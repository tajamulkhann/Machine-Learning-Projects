{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Anomaly Detection Demo\n",
    "\n",
    "This notebook demonstrates end-to-end unsupervised anomaly detection on a made-up transaction dataset using Isolation Forest and Local Outlier Factor (LOF), with rich comments for all complex terms and hyperparameters to serve as a learning reference and a project template [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install packages if needed and import libraries used throughout the notebook [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If running in a fresh environment, uncomment:\n",
    "# !pip install scikit-learn numpy pandas matplotlib seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler  # Robust to outliers via median/IQR scaling [attached_file:1][attached_file:2]\n",
    "from sklearn.ensemble import IsolationForest     # Tree-based outlier detector isolating anomalies faster [attached_file:1][attached_file:2]\n",
    "from sklearn.neighbors import LocalOutlierFactor # Density-based outlier detector using local neighbor density [attached_file:1][attached_file:2]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data synthesis\n",
    "\n",
    "Create a synthetic transactions dataset with plausible behavioral and risk features; inject a small fraction of anomalies to emulate fraud-like behavior for evaluation [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)  # Reproducible randomness [attached_file:1][attached_file:2]\n",
    "\n",
    "n_users = 500         # Number of distinct users [attached_file:1][attached_file:2]\n",
    "n_txn = 8000          # Number of transactions [attached_file:1][attached_file:2]\n",
    "\n",
    "# Synthetic user IDs\n",
    "user_ids = rng.integers(10000, 10000 + n_users, size=n_txn)  # Random assignment of users per txn [attached_file:1][attached_file:2]\n",
    "\n",
    "# Random timestamps within a 30-day window\n",
    "start = datetime(2025, 1, 1)  # Arbitrary start date [attached_file:1][attached_file:2]\n",
    "timestamps = [start + timedelta(minutes=int(x))\n",
    "              for x in rng.integers(0, 30*24*60, size=n_txn)]  # Uniform spread over 30 days [attached_file:1][attached_file:2]\n",
    "\n",
    "# Base numeric features\n",
    "amount = np.clip(rng.lognormal(mean=3.2, sigma=0.8, size=n_txn), 1, None)  # Right-skew; lognormal simulates spend [attached_file:1][attached_file:2]\n",
    "txn_hour = np.array([ts.hour for ts in timestamps])                          # Hour of day (0-23) [attached_file:1][attached_file:2]\n",
    "device_age_days = np.clip(rng.normal(365, 120, size=n_txn), 1, None)         # Device tenure; older often safer [attached_file:1][attached_file:2]\n",
    "geo_distance_km = np.clip(rng.gamma(shape=2.0, scale=10.0, size=n_txn), 0, None)  # Approx travel jump [attached_file:1][attached_file:2]\n",
    "merchant_risk_score = rng.choice([0.1, 0.2, 0.4, 0.6, 0.8], \n",
    "                                 p=[0.25, 0.30, 0.25, 0.15, 0.05], size=n_txn)  # Higher means riskier merchant [attached_file:1][attached_file:2]\n",
    "\n",
    "# Velocity proxy: count-like behavior; transactions clustering increase velocity [attached_file:1][attached_file:2]\n",
    "base_velocity = np.clip((amount > 50).astype(int) + rng.poisson(0.5, size=n_txn), 0, None)  # Simple heuristic [attached_file:1][attached_file:2]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"user_id\": user_ids,\n",
    "    \"timestamp\": timestamps,\n",
    "    \"amount\": amount,\n",
    "    \"txn_hour\": txn_hour,\n",
    "    \"device_age_days\": device_age_days,\n",
    "    \"geo_distance_km\": geo_distance_km,\n",
    "    \"merchant_risk_score\": merchant_risk_score,\n",
    "    \"velocity_approx\": base_velocity\n",
    "})  # Core event table [attached_file:1][attached_file:2]\n",
    "\n",
    "df.sort_values([\"user_id\", \"timestamp\"], inplace=True, ignore_index=True)  # Needed for rolling features [attached_file:1][attached_file:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject anomalies\n",
    "\n",
    "Inject extreme behaviors (large amounts, odd hours, long geo jumps, high velocity, risky merchants) for a small fraction of transactions to simulate fraud-like outliers [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "anomaly_fraction = 0.02  # Expected anomaly rate; used later as contamination hint [attached_file:1][attached_file:2]\n",
    "n_anom = int(n_txn * anomaly_fraction)  # Number of rows to flip to anomalies [attached_file:1][attached_file:2]\n",
    "anom_idx = rng.choice(df.index, size=n_anom, replace=False)  # Random selection [attached_file:1][attached_file:2]\n",
    "\n",
    "# Apply extreme patterns to chosen rows [attached_file:1][attached_file:2]\n",
    "df.loc[anom_idx, \"amount\"] = df.loc[anom_idx, \"amount\"] * rng.uniform(5, 20, size=n_anom)  # Inflate spend [attached_file:1][attached_file:2]\n",
    "df.loc[anom_idx, \"txn_hour\"] = rng.choice([1, 2, 3, 4], size=n_anom)  # Odd hours cluster [attached_file:1][attached_file:2]\n",
    "df.loc[anom_idx, \"geo_distance_km\"] = df.loc[anom_idx, \"geo_distance_km\"] + rng.uniform(2000, 10000, size=n_anom)  # Long jumps [attached_file:1][attached_file:2]\n",
    "df.loc[anom_idx, \"merchant_risk_score\"] = 0.8  # Highest bucket [attached_file:1][attached_file:2]\n",
    "df.loc[anom_idx, \"velocity_approx\"] = df.loc[anom_idx, \"velocity_approx\"] + rng.integers(5, 20, size=n_anom)  # Velocity burst [attached_file:1][attached_file:2]\n",
    "\n",
    "df[\"is_injected_anomaly\"] = 0  # Proxy label, only for evaluation in this demo [attached_file:1][attached_file:2]\n",
    "df.loc[anom_idx, \"is_injected_anomaly\"] = 1  # Mark injected anomalies [attached_file:1][attached_file:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling features per user\n",
    "\n",
    "Compute short-history behavior per user via rolling windows to capture context (means/stds over last events), which helps unsupervised models separate unusual behavior from each user's baseline [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_user_rollups(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = g.sort_values(\"timestamp\")  # Ensure temporal order [attached_file:1][attached_file:2]\n",
    "    # Rolling window size = 7 events within each user; min_periods=1 allows early rows [attached_file:1][attached_file:2]\n",
    "    g[\"amt_mean_7\"] = g[\"amount\"].rolling(7, min_periods=1).mean()  # Local spending baseline [attached_file:1][attached_file:2]\n",
    "    g[\"amt_std_7\"] = g[\"amount\"].rolling(7, min_periods=1).std().fillna(0.0)  # Volatility proxy [attached_file:1][attached_file:2]\n",
    "    g[\"velocity_mean_7\"] = g[\"velocity_approx\"].rolling(7, min_periods=1).mean()  # Habitual burstiness [attached_file:1][attached_file:2]\n",
    "    g[\"geo_mean_7\"] = g[\"geo_distance_km\"].rolling(7, min_periods=1).mean()  # Usual travel distance [attached_file:1][attached_file:2]\n",
    "    return g  # Return with added columns [attached_file:1][attached_file:2]\n",
    "\n",
    "df = df.groupby(\"user_id\", group_keys=False).apply(add_user_rollups)  # Efficient per-user transform [attached_file:1][attached_file:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform and scale\n",
    "\n",
    "Use log1p to reduce skew on heavy-tailed features and RobustScaler to reduce sensitivity to outliers when standardizing features going into detectors [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Log-transform skewed features; log1p(x) = log(1+x) is defined at 0 and reduces heavy tails [attached_file:1][attached_file:2]\n",
    "for col in [\"amount\", \"velocity_approx\", \"geo_distance_km\", \"amt_mean_7\", \"amt_std_7\", \"velocity_mean_7\", \"geo_mean_7\"]:\n",
    "    df[f\"log1p_{col}\"] = np.log1p(df[col])  # Adds transformed columns in place [attached_file:1][attached_file:2]\n",
    "\n",
    "feature_cols = [\n",
    "    \"log1p_amount\",\n",
    "    \"txn_hour\",\n",
    "    \"device_age_days\",\n",
    "    \"log1p_geo_distance_km\",\n",
    "    \"merchant_risk_score\",\n",
    "    \"log1p_velocity_approx\",\n",
    "    \"log1p_amt_mean_7\",\n",
    "    \"log1p_amt_std_7\",\n",
    "    \"log1p_velocity_mean_7\",\n",
    "    \"log1p_geo_mean_7\"\n",
    "]  # Balanced mix of raw and behavioral signals [attached_file:1][attached_file:2]\n",
    "\n",
    "X = df[feature_cols].copy()  # Model input matrix [attached_file:1][attached_file:2]\n",
    "\n",
    "scaler = RobustScaler()  # Uses median and IQR; less distorted by outliers than StandardScaler [attached_file:1][attached_file:2]\n",
    "X_scaled = scaler.fit_transform(X)  # Fit on all data (unsupervised) [attached_file:1][attached_file:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train detectors\n",
    "\n",
    "Fit Isolation Forest and Local Outlier Factor on the scaled features; explain critical hyperparameters and outputs for each method [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Isolation Forest: tree-based method that isolates points by random splits; anomalies isolate faster (shorter paths) [attached_file:1][attached_file:2]\n",
    "iso = IsolationForest(\n",
    "    n_estimators=300,        # Number of trees; more trees -> more stable scores but slower [attached_file:1][attached_file:2]\n",
    "    contamination=0.02,      # Approx fraction of anomalies; tunes threshold automatically [attached_file:1][attached_file:2]\n",
    "    max_samples=\"auto\",      # Subsample size per tree; \"auto\"=min(256, n_samples) for efficiency [attached_file:1][attached_file:2]\n",
    "    random_state=42,\n",
    "    n_jobs=-1                # Use all cores for speed [attached_file:1][attached_file:2]\n",
    ")\n",
    "iso_pred = iso.fit_predict(X_scaled)        # Outputs 1 for normal, -1 for anomaly [attached_file:1][attached_file:2]\n",
    "iso_score = iso.decision_function(X_scaled) # Higher scores -> more normal; lower -> more anomalous [attached_file:1][attached_file:2]\n",
    "\n",
    "# Local Outlier Factor: density-based; compares local density of a point to its neighbors (lower density => outlier) [attached_file:1][attached_file:2]\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=35,          # Size of local neighborhood; too small => noisy, too large => oversmooth [attached_file:1][attached_file:2]\n",
    "    contamination=0.02,      # Used to set threshold on the LOF scores [attached_file:1][attached_file:2]\n",
    "    metric=\"minkowski\",      # Default distance; Euclidean when p=2 [attached_file:1][attached_file:2]\n",
    "    novelty=False,           # Fit+predict on same data; set True to use .predict on new data after .fit [attached_file:1][attached_file:2]\n",
    "    n_jobs=-1\n",
    ")\n",
    "lof_pred = lof.fit_predict(X_scaled)        # 1 for inliers, -1 for outliers [attached_file:1][attached_file:2]\n",
    "lof_score = lof.negative_outlier_factor_    # More negative => more anomalous [attached_file:1][attached_file:2]\n",
    "\n",
    "# Collect results\n",
    "df_results = df.copy()\n",
    "df_results[\"iso_label\"] = (iso_pred == -1).astype(int)  # 1=anomaly, 0=normal for convenience [attached_file:1][attached_file:2]\n",
    "df_results[\"iso_score\"] = iso_score\n",
    "df_results[\"lof_label\"] = (lof_pred == -1).astype(int)\n",
    "df_results[\"lof_score\"] = lof_score\n",
    "\n",
    "# Simple ensemble: flag if either detector says anomaly (OR rule) [attached_file:1][attached_file:2]\n",
    "df_results[\"ensemble_label_or\"] = ((df_results[\"iso_label\"] + df_results[\"lof_label\"]) > 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding notes\n",
    "\n",
    "Both models used a contamination of 2%, which sets internal thresholds; in real scenarios, tune thresholds to desired precision/recall, alert volume, and operational cost constraints [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with proxy labels\n",
    "\n",
    "Use the injected anomaly indicator as a proxy label for rough evaluation of detector quality; compute basic precision/recall and confusion matrices [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true = df_results[\"is_injected_anomaly\"].values  # Proxy ground truth [attached_file:1][attached_file:2]\n",
    "\n",
    "def report(name, y_pred):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(confusion_matrix(y_true, y_pred))  # [[TN FP]\n",
    "                                             #  [FN TP]] [attached_file:1][attached_file:2]\n",
    "    print(classification_report(y_true, y_pred, digits=4))  # Precision/Recall/F1 [attached_file:1][attached_file:2]\n",
    "\n",
    "report(\"IsolationForest\", df_results[\"iso_label\"].values)\n",
    "report(\"LocalOutlierFactor\", df_results[\"lof_label\"].values)\n",
    "report(\"Ensemble_OR\", df_results[\"ensemble_label_or\"].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick inspection\n",
    "\n",
    "Glance at a few anomalies to validate plausibility and inspect features contributing to flags [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "anoms = df_results[df_results[\"ensemble_label_or\"] == 1].copy()\n",
    "anoms_sorted = anoms.sort_values([\"iso_label\", \"lof_label\", \"amount\"], ascending=[False, False, False])\n",
    "anoms_sorted.head(10)  # Inspect top anomalies by amount and flags [attached_file:1][attached_file:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D visualization (PCA)\n",
    "\n",
    "Project features into 2D using PCA to visualize separation between normal points and anomalies; this is illustrative only, since high-dimensional structure is compressed [attached_file:1][attached_file:2]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA  # Linear dimensionality reduction via principal components [attached_file:1][attached_file:2]\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)  # 2D for plotting [attached_file:1][attached_file:2]\n",
    "X_2d = pca.fit_transform(X_scaled)  # Transform full dataset [attached_file:1][attached_file:2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "mask_anom = df_results[\"ensemble_label_or\"].values == 1\n",
    "ax.scatter(X_2d[~mask_anom, 0], X_2d[~mask_anom, 1], s=10, c=\"#4caf50\", alpha=0.5, label=\"Normal\")\n",
    "ax.scatter(X_2d[mask_anom, 0], X_2d[mask_anom, 1], s=20, c=\"#e53935\", alpha=0.8, label=\"Anomaly\")\n",
    "ax.set_title(\"PCA projection: anomalies vs normal\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.legend()\n",
    "plt.show()  # Visual check of separation [attached_file:1][attached_file:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on deployment\n",
    "\n",
    "- Isolation Forest can serve scores in real time; re-train periodically as behavior shifts (concept drift) [attached_file:1][attached_file:2].\n",
    "- LOF with novelty=True supports scoring on new data after fitting; for streaming, fit on a clean reference window and then predict on incoming events [attached_file:1][attached_file:2].\n",
    "- Thresholds should be calibrated to business KPIs (alert budgets, investigation cost, expected fraud base rate) using a validation set or human-in-the-loop feedback [attached_file:1][attached_file:2]."
   ]
  }
 ]
}
